# OMatG: Open Materials Generation

![csp movie](assets/csp_movie.gif)

A state-of-the-art generative model for crystal structure prediction and *de novo* generation of inorganic crystals. 

This open-source framework accompanies the paper available on [arXiv](https://arxiv.org/abs/2502.02582),
which should be cited when using it.

> **NOTE:** ðŸš§ This repository is currently work in progress. ðŸš§

## Table of Contents

- [Dependencies](#dependencies)
- [Examples](#examples)
- [Training](#training)
- [Sampling](#sampling)
- [Visualize](#visualize)
- [Match Rate Computation](#match-rate-csp)
- [Alex-MP20 Dataset](#alex-mp20-dataset)
- [OMatG Data Format](#omatg-data-format)

## Dependencies

You can use any Python version between 3.10 and 3.12.

Install the dependencies and the `omg` package itself via pip. 

To install `omg` in editable mode (recommended for developers), use `pip install -e .` within the base directory of this 
repository once. Any changes in code are directly available in the installed `omg` package. 

> **NOTE**: Installing `torch=2.7.1` based on your version of `CUDA` in addition to `torch_geometric`, and `torch-scatter` before installing `omg` can help minimize sources of errors

To install `omg` as a package, use `pip install .` instead. If the code in this repository changes, this command has to 
executed again to also change the code of the installed package.

## Examples

Tutorial notebooks for using OMatG are available on [Kaggle](https://www.kaggle.com/philipphoellmer/code). In particular the [Crystals](https://www.kaggle.com/code/philipphoellmer/generative-modeling-workshop-session-crystals-sol) and [Crystals Sol](https://www.kaggle.com/code/philipphoellmer/generative-modeling-workshop-session-crystals-sol) notebooks are relevant.

## Training


Run the following command in any directory to train from scratch based on the configuration file `config.yaml`:

```bash
omg fit --config config.yaml --trainer.accelerator=gpu
```

This command will create checkpoints, log files, and cache files in the working directory.

If you want to include a Wandb logger with a name, add the `--trainer.logger=WandbLogger --trainer.logger.name=<name>` 
argument. Other loggers can be found [here](https://lightning.ai/docs/pytorch/stable/extensions/logging.html).

In order to restart training from a checkpoint, add the `--ckpt_path=<checkpoint_file.ckpt>` argument. 

In order to seed the random number generators before training, use `--seed_everything=<seed>`.

Exemplary configuration files can be found in the `omg/conf_examples` directory.

The training command can be executed in any directory. The configuration files contain paths to lmbd data files that are 
used, e.g., for training. The path to these data files can either be relative to the working directory, or relative to 
the `omg` directory (that is, use `"data/mp_20/val.lmdb"` for `lmdb_paths` in order to use the `mp_20` data set as in 
exemplary configuration files).

> **NOTE**: Model checkpoints used in the paper can be found at the [HuggingFace link](https://huggingface.co/OMatG) associated with OMatG. See details on [generating materials](#sampling) with checkpoint files below.


## Sampling

For generating new structures in an xyz file, run the following command:

```bash
omg predict --config {config_file} --ckpt_path=<checkpoint_file.ckpt> --model.generation_xyz_filename=<xyz_file> --data.batch_size=1024 --seed_everything=42 --trainer.max_epochs=1
```

For an xyz filename `filename.xyz`, this command will also create a file `filename_init.xyz` that contains the initial
structures that were integrated to yield the structures in `filename.xyz`. This file is required for the visualization
below.

## Visualize

Run the following command to compare distributions over the generated structures in an xyz file to distributions over 
training dataset:

```bash
omg visualize --config {config_file} --xyz_file {xyz_file} --plot_name {plot_name}
```

## Match Rate (CSP)

Run the following command to compute the match rate between the generated structures in an xyz file and the structures 
in the prediction dataset, as well as the rate of unique structures in the generated structures:

```bash
omg match --config {config_file} --xyz_file {xyz_file}
```

Computing the match rate as in DiffCSP or FlowMM requires to validate every structure which is quite slow. Also, 
computing the unique rate is slow. One can use the `--skip_validation=true` and `--skip_unique=true` arguments to skip
these computations. 

The validations, and the computations of the match rate and unique rate are parallelized. The number of processes is 
determined by `os.cpu_count()`. This can be changed by setting the `--number_cpus` argument (which is probably most 
useful in cluster environments).

## Alex-MP20 dataset

The Alex-MP20 dataset is too large to be stored in this repository. We have made it available via the [HuggingFace link](https://huggingface.co/OMatG) associated with this project.

<!---
## Curriculum Learning

Run the following command to use a given configuration file with stochastic interpolants for all datafields 'pos', 
'species', and 'cell' to generate a new configuration file where only some of the stochastic interpolants are used, 
while the others are replaced by identity interpolants:

```bash
omg curriculum --config {config_file} --lessons {lessons}
```

Here, lessons should be a list of strings so, e.g., [pos] or [pos,species] (spaces can be included when the brackets are
surrounded by quotes).
--->
## OMatG Data Format

For a batch size of batch_size, the `torch_geometric.data.Data` instances contain the following attributes:
- `n_atoms`: `torch.Tensor` of shape `(batch_size, )` containing the number of atoms in each configuration.
- `batch`: `torch.Tensor` of shape `(sum(n_atoms),)` containing the index of the configuration to which each atom 
belongs.
- `species`: `torch.Tensor` of shape `(sum(n_atoms),)` containing the atomic numbers of the atoms in the configurations.
- `pos`: `torch.Tensor` of shape `(sum(n_atoms), 3)` containing the atomic positions of the atoms in the configurations.
- `cell`: `torch.Tensor` of shape `(batch_size, 3, 3)` containing the cell vectors of the configurations.
- `ptr`: `torch.Tensor` of shape `(batch_size + 1,)` containing the indices of the first atom of each configuration in 
the `species` and `pos` tensors.
- `property`: dict containing the properties of the configurations.
